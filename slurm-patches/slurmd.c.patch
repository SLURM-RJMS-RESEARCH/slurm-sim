*** slurm/src/slurmd/slurmd/slurmd.c	2013-08-28 18:04:01.358682686 +0200
--- ../sim_test_dir/slurm/src/slurmd/slurmd/slurmd.c	2013-02-26 11:04:30.000000000 +0100
***************
*** 101,106 ****
--- 101,109 ----
  #include "src/slurmd/slurmd/get_mach_stat.h"
  #include "src/slurmd/common/proctrack.h"
  
+  
+ #include "sim_events.h"
+  
  #define GETOPT_ARGS	"cCd:Df:hL:Mn:N:vV"
  
  #ifndef MAXHOSTNAMELEN
***************
*** 127,132 ****
--- 130,140 ----
  	slurm_addr_t *cli_addr;
  } conn_t;
  
+ volatile simulator_event_t *head_simulator_event;
+ volatile simulator_event_t *head_sim_completed_jobs;
+ int total_sim_events = 0;
+ 
+ pthread_mutex_t simulator_mutex  = PTHREAD_MUTEX_INITIALIZER;
  
  
  /*
***************
*** 151,156 ****
--- 159,168 ----
  static void      _install_fork_handlers(void);
  static void 	 _kill_old_slurmd(void);
  static void      _msg_engine(void);
+ #ifdef SLURM_SIMULATOR
+ static void     *_simulator_helper(void *arg);
+ static void      _spawn_simulator_helper(void);
+ #endif
  static void      _print_conf(void);
  static void      _print_config(void);
  static void      _process_cmdline(int ac, char **av);
***************
*** 316,321 ****
--- 328,336 ----
  	slurm_conf_install_fork_handlers();
  
  	_spawn_registration_engine();
+ #ifdef SLURM_SIMULATOR
+     	_spawn_simulator_helper();
+ #endif
  	_msg_engine();
  
  	/*
***************
*** 341,346 ****
--- 356,388 ----
  }
  
  static void
+ _spawn_simulator_helper(void)
+ {
+ 	int            rc;
+ 	pthread_attr_t attr;
+ 	pthread_t      id;
+ 	int            retries = 0;
+ 
+ 	slurm_attr_init(&attr);
+ 	rc = pthread_attr_setdetachstate(&attr, PTHREAD_CREATE_DETACHED);
+ 	if (rc != 0) {
+ 		errno = rc;
+ 		fatal("Unable to set detachstate on attr: %m");
+ 		slurm_attr_destroy(&attr);
+ 		return;
+ 	}
+ 
+ 	while (pthread_create(&id, &attr, &_simulator_helper, NULL)) {
+ 		error("simulator_helper: pthread_create: %m");
+ 		if (++retries > 3)
+ 			fatal("simulator_helper: pthread_create: %m");
+ 		usleep(10);	/* sleep and again */
+ 	}
+ 
+ 	return;
+ }
+ 
+ void
  _spawn_registration_engine(void)
  {
  	int            rc;
***************
*** 388,396 ****
--- 430,586 ----
  	}
  
  	_decrement_thd_count();
+ 
+ 	pthread_exit(NULL);
+ 
  	return NULL;
  }
  
+ static int
+ _send_complete_batch_script_msg(uint32_t jobid, int err, int status)
+ {
+ 	int		rc, i;
+ 	slurm_msg_t	req_msg;
+ 	complete_batch_script_msg_t req;
+ 
+ 	req.job_id	= jobid;
+ 	req.job_rc      = status;
+ 	req.slurm_rc	= err;
+ 
+ 	slurm_msg_t_init(&req_msg);
+ 	req.node_name	= NULL;
+ 	req_msg.msg_type= REQUEST_COMPLETE_BATCH_SCRIPT;
+ 	req_msg.data	= &req;
+ 
+ 	info("SIM: sending REQUEST_COMPLETE_BATCH_SCRIPT");
+ 
+ 	/* Note: these log messages don't go to slurmd.log from here */
+ 	for (i=0; i<=5; i++) {
+ 		struct timespec waiting;
+ 
+ 		if (slurm_send_recv_controller_rc_msg(&req_msg, &rc) == 0)
+ 			break;
+ 		info("SIM: Retrying job complete RPC for %u",
+ 		     jobid);
+ 		waiting.tv_sec = 0;
+ 		waiting.tv_nsec = 10000000;
+ 		//usleep(10000);
+ 		nanosleep(&waiting, 0);
+ 	}
+ 	if (i > 5) {
+ 		sleep(10);
+ 		error("SIM: Unable to send job complete message: %m");
+ 		return SLURM_ERROR;
+ 	}
+ 
+ 	if ((rc == ESLURM_ALREADY_DONE) || (rc == ESLURM_INVALID_JOB_ID))
+ 		rc = SLURM_SUCCESS;
+ 	if (rc)
+ 		slurm_seterrno_ret(rc);
+ 
+ 	return SLURM_SUCCESS;
+ }
+ 
+ static int
+ _send_sim_helper_cycle_msg(uint32_t jobs_count)
+ {
+ 	int		rc, i;
+ 	slurm_msg_t	req_msg;
+ 	sim_helper_msg_t req;
+ 
+ 	req.total_jobs_ended = jobs_count;
+ 
+ 	slurm_msg_t_init(&req_msg);
+ 	req_msg.msg_type= MESSAGE_SIM_HELPER_CYCLE;
+ 	req_msg.data	= &req;
+ 
+ 	info("SIM: sending MESSAGE_SIM_HELPER_CYCLE");
+ 
+ 	/* Note: these log messages don't go to slurmd.log from here */
+ 	for (i=0; i<=5; i++) {
+ 		struct timespec waiting;
+ 
+ 		if (slurm_send_recv_controller_rc_msg(&req_msg, &rc) == 0)
+ 			break;
+ 		info("SIM: Retrying message helper cycle RPC");
+ 		waiting.tv_sec = 0;
+ 		waiting.tv_nsec = 10000000;
+ 		//usleep(10000);
+ 		nanosleep(&waiting, 0);
+ 	}
+ 	if (i > 5) {
+ 		sleep(10);
+ 		error("SIM: Unable to send message helper cycle complete message: %m");
+ 		return SLURM_ERROR;
+ 	}
+ 
+ 	if ((rc == ESLURM_ALREADY_DONE) || (rc == ESLURM_INVALID_JOB_ID))
+ 		rc = SLURM_SUCCESS;
+ 	if (rc)
+ 		slurm_seterrno_ret(rc);
+ 
+ 	return SLURM_SUCCESS;
+ }
+ 
+ 
+ void *
+ _simulator_helper(void *arg)
+ {
+    time_t now, last;
+    int jobs_ended;
+ 	_increment_thd_count();
+ 
+     last = 0;
+     now = 0;
+    info("SIM: Simulator Helper starting...\n");
+ 	while (!_shutdown) {
+ 
+ 		jobs_ended = 0;
+ 		now = time(NULL);
+ 		if((now - last != 1) && (last > 0)){
+ 			info("Simulator Helper cycle ERROR: last %ld and now %ld\n", last, now);
+ 			while(1);
+ 		}
+ 		pthread_mutex_lock(&simulator_mutex);
+ 		if(head_simulator_event)
+ 			info("Simulator Helper cycle: %ld, Next event at %ld, total_sim_events: %d\n", now, head_simulator_event->when, total_sim_events);
+ 		else
+ 			info("Simulator Helper cycle: %ld, No events!!!\n", now);
+ 
+ 		while((head_simulator_event) && (now >= head_simulator_event->when)){
+ 			simulator_event_t *aux;
+ 			int event_jid;
+ 			event_jid = head_simulator_event->job_id;
+ 			aux = head_simulator_event;
+ 			head_simulator_event = head_simulator_event->next;
+ 			aux->next = head_sim_completed_jobs;
+ 			head_sim_completed_jobs = aux;
+ 			total_sim_events--;
+ 			info("SIM: Sending JOB_COMPLETE_BATCH_SCRIPT for job %d", event_jid);
+ 			pthread_mutex_unlock(&simulator_mutex);
+ 			_send_complete_batch_script_msg(event_jid, SLURM_SUCCESS, 0);
+ 			pthread_mutex_lock(&simulator_mutex);
+ 			info("SIM: JOB_COMPLETE_BATCH_SCRIPT for job %d SENT", event_jid);
+ 			jobs_ended++;
+ 			
+ 		}
+ 		pthread_mutex_unlock(&simulator_mutex);
+ 		last = now;
+ 		if(jobs_ended){
+ 			/* Let's give some time to EPILOG_MESSAGE process to terminate  */
+ 			/* TODO: It should be done better with a counter of EPILOG messages processed */
+ 			usleep(5000);
+ 			_send_sim_helper_cycle_msg(jobs_ended);
+ 		}
+ 		sleep(1);
+ 	
+ 	}
+   	info("SIM: Simulator Helper finishing...");
+ 	pthread_exit(0);
+ 	_decrement_thd_count();
+  	return NULL;
+ }
+ 
  static void
  _msg_engine(void)
  {
***************
*** 420,425 ****
--- 610,618 ----
  	}
  	verbose("got shutdown request");
  	slurm_shutdown_msg_engine(conf->lfd);
+ 
+ 	pthread_exit(NULL);
+ 
  	return;
  }
  
***************
*** 478,484 ****
  	verbose("all threads complete");
  }
  
! static void
  _handle_connection(slurm_fd_t fd, slurm_addr_t *cli)
  {
  	int            rc;
--- 671,677 ----
  	verbose("all threads complete");
  }
  
! void
  _handle_connection(slurm_fd_t fd, slurm_addr_t *cli)
  {
  	int            rc;
***************
*** 549,554 ****
--- 742,750 ----
  	xfree(con);
  	slurm_free_msg(msg);
  	_decrement_thd_count();
+ 
+ 	pthread_exit(NULL);
+ 
  	return NULL;
  }
  
***************
*** 1361,1377 ****
  	}
  #endif /* !NDEBUG */
  
  	/*
  	 * Create a context for verifying slurm job credentials
  	 */
  	if (!(conf->vctx = slurm_cred_verifier_ctx_create(conf->pubkey)))
  		return SLURM_FAILURE;
  	if (!strcmp(conf->select_type, "select/serial")) {
  		/* Only cache credential for 5 seconds with select/serial
  		 * for shorter cache searches and higher throughput */
  		slurm_cred_ctx_set(conf->vctx, SLURM_CRED_OPT_EXPIRY_WINDOW, 5);
  	}
! 
  	/*
  	 * Create slurmd spool directory if necessary.
  	 */
--- 1557,1575 ----
  	}
  #endif /* !NDEBUG */
  
+ #ifndef SLURM_SIMULATOR
  	/*
  	 * Create a context for verifying slurm job credentials
  	 */
  	if (!(conf->vctx = slurm_cred_verifier_ctx_create(conf->pubkey)))
  		return SLURM_FAILURE;
+ klqwjelkqwj
  	if (!strcmp(conf->select_type, "select/serial")) {
  		/* Only cache credential for 5 seconds with select/serial
  		 * for shorter cache searches and higher throughput */
  		slurm_cred_ctx_set(conf->vctx, SLURM_CRED_OPT_EXPIRY_WINDOW, 5);
  	}
! #endif
  	/*
  	 * Create slurmd spool directory if necessary.
  	 */
