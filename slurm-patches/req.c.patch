*** slurm/src/slurmd/slurmd/req.c	2013-08-28 18:04:01.358682686 +0200
--- ../sim_test_dir/slurm/src/slurmd/slurmd/req.c	2012-11-29 16:48:17.000000000 +0100
***************
*** 92,97 ****
--- 92,99 ----
  #include "src/slurmd/common/slurmstepd_init.h"
  #include "src/slurmd/common/task_plugin.h"
  
+ #include "sim_events.h"
+ 
  #define _LIMIT_INFO 0
  
  #define RETRY_DELAY 15		/* retry every 15 seconds */
***************
*** 156,162 ****
--- 158,167 ----
  static void _rpc_signal_job(slurm_msg_t *);
  static void _rpc_suspend_job(slurm_msg_t *msg, int version);
  static void _rpc_terminate_job(slurm_msg_t *);
+ static void _rpc_simulator_terminate_job(slurm_msg_t *);
+ static int _rpc_sim_job(slurm_msg_t *msg);
  static void _rpc_update_time(slurm_msg_t *);
+ static void _rpc_simulator_batch_job(slurm_msg_t *);
  static void _rpc_shutdown(slurm_msg_t *msg);
  static void _rpc_reconfig(slurm_msg_t *msg);
  static void _rpc_reboot(slurm_msg_t *msg);
***************
*** 204,209 ****
--- 209,217 ----
  static int  _compare_job_running_prolog(void *s0, void *s1);
  static void _wait_for_job_running_prolog(uint32_t job_id);
  
+ static void simulator_rpc_batch_job(slurm_msg_t *msg);
+ static void simulator_rpc_terminate_job(slurm_msg_t *rec_msg);
+ 
  /*
   *  List of threads waiting for jobs to complete
   */
***************
*** 222,227 ****
--- 230,242 ----
  static uint32_t fini_job_id[FINI_JOB_CNT];
  static int next_fini_job_inx = 0;
  
+ extern pthread_mutex_t simulator_mutex;
+ extern simulator_event_t *head_simulator_event;
+ extern simulator_event_t *head_sim_completed_jobs;
+ extern int total_sim_events;
+ 
+ simulator_event_info_t *head_simulator_event_info;
+ 
  /* NUM_PARALLEL_SUSPEND controls the number of jobs suspended/resumed
   * at one time as well as the number of jobsteps per job that can be
   * suspended at one time */
***************
*** 254,265 ****
--- 269,285 ----
  
  	switch(msg->msg_type) {
  	case REQUEST_BATCH_JOB_LAUNCH:
+ #ifndef SLURM_SIMULATOR
  		debug2("Processing RPC: REQUEST_BATCH_JOB_LAUNCH");
  		/* Mutex locking moved into _rpc_batch_job() due to
  		 * very slow prolog on Blue Gene system. Only batch
  		 * jobs are supported on Blue Gene (no job steps). */
  		_rpc_batch_job(msg, true);
  		last_slurmctld_msg = time(NULL);
+ #else
+ 		simulator_rpc_batch_job(msg);
+ #endif
+ 		info("REQUEST BATCH_JOB_LAUNCH FINISHED\n");
  		slurm_free_job_launch_msg(msg->data);
  		break;
  	case REQUEST_LAUNCH_TASKS:
***************
*** 329,335 ****
--- 349,360 ----
  	case REQUEST_TERMINATE_JOB:
  		debug2("Processing RPC: REQUEST_TERMINATE_JOB");
  		last_slurmctld_msg = time(NULL);
+ #ifndef SLURM_SIMULATOR
  		_rpc_terminate_job(msg);
+ #else
+ 		simulator_rpc_terminate_job(msg);
+ #endif
+ 		info("RPC TERMINATE_JOB FINISHED\n");
  		slurm_free_kill_job_msg(msg->data);
  		break;
  	case REQUEST_COMPLETE_BATCH_SCRIPT:
***************
*** 419,424 ****
--- 444,453 ----
  		_rpc_forward_data(msg);
  		slurm_free_forward_data_msg(msg->data);
  		break;
+ 	case REQUEST_SIM_JOB:
+         info("SIM: REQUEST_SIM_JOB message received\n");
+ 		_rpc_sim_job(msg);
+ 		break;
  	default:
  		error("slurmd_req: invalid request msg type %d",
  		      msg->msg_type);
***************
*** 441,446 ****
--- 470,571 ----
  	return (-1);
  }
  
+ #ifdef SLURM_SIMULATOR
+ 
+ int simulator_add_future_event(batch_job_launch_msg_t *req){
+     simulator_event_t  *new_event;
+     simulator_event_info_t *temp_ptr = head_simulator_event_info;
+     time_t now;
+ 
+     pthread_mutex_lock(&simulator_mutex);
+     now = time(NULL);
+ 
+     new_event = (simulator_event_t *)malloc(sizeof(simulator_event_t));
+     if(!new_event){
+         error("SIMULATOR: malloc fails for new_event\n");
+ 	   	pthread_mutex_unlock(&simulator_mutex);
+        return -1;
+     }
+ 
+     /* Checking job_id as expected */
+     while(temp_ptr){
+         if(temp_ptr->job_id == req->job_id)
+             break;
+         temp_ptr = temp_ptr->next;
+     }
+     if(!temp_ptr){
+         info("SIM: No job_id event matching this job_id %d\n", req->job_id);
+ 		pthread_mutex_unlock(&simulator_mutex);
+        return -1;
+     }
+     new_event->job_id = req->job_id;
+     new_event->type = REQUEST_COMPLETE_BATCH_SCRIPT;
+     new_event->when = now + temp_ptr->duration;
+     new_event->nodelist = strdup(req->nodes);
+     new_event->next = NULL;
+ 
+     total_sim_events++;
+     if(!head_simulator_event){
+         info("SIM: Adding new event for job %d when list is empty for future time %ld!", new_event->job_id, new_event->when);
+         head_simulator_event = new_event;
+     }else{
+ 		simulator_event_t *node_temp = head_simulator_event;
+         info("SIM: Adding new event for job %d in the event listi for future time %ld", new_event->job_id, new_event->when);
+ 
+         if(head_simulator_event->when > new_event->when){
+ 				new_event->next = head_simulator_event;
+ 				head_simulator_event = new_event;
+ 				pthread_mutex_unlock(&simulator_mutex);
+ 				return 0;
+ 		}
+ 	
+ 		while((node_temp->next) && (node_temp->next->when < new_event->when))
+ 			node_temp = node_temp->next;
+ 
+ 		if(node_temp->next){
+ 				new_event->next = node_temp->next;
+ 				node_temp->next = new_event;
+ 				pthread_mutex_unlock(&simulator_mutex);
+ 				return 0;
+ 		}
+ 		node_temp->next = new_event;
+   }
+ 
+ 	pthread_mutex_unlock(&simulator_mutex);
+ 	return 0;
+ }
+ 
+ static void
+ simulator_rpc_batch_job(slurm_msg_t *msg)
+ {
+ 	batch_job_launch_msg_t *req = (batch_job_launch_msg_t *)msg->data;
+ 	bool     first_job_run = true;
+ 	int      rc = SLURM_SUCCESS;
+ 	uid_t    req_uid = g_slurm_auth_get_uid(msg->auth_cred, NULL);
+ 	char    *resv_id = NULL;
+ 	bool	 replied = false;
+ 	slurm_addr_t *cli = &msg->orig_addr;
+    hostlist_t hl;
+     char *node_name;
+ 
+     info("SIM: Dentro de simulator_rpc_batch_job for %d\n", req->job_id);
+     hl = hostlist_create(req->nodes);
+     while ((node_name = hostlist_shift(hl))) {
+         info("SIM: nodelist %s\n", node_name);
+     }
+     info("SIM: Hostlist printed\n");
+ 
+ 
+     if (slurm_send_rc_msg(msg, SLURM_SUCCESS) < 1) {
+ 	error("SIM: Could not confirm batch launch for job %d\n", req->job_id);
+    }
+ 
+     simulator_add_future_event(req);
+ 
+ 
+ }
+ #endif
+ 
  static int
  _send_slurmstepd_init(int fd, slurmd_step_type_t type, void *req,
  		      slurm_addr_t *cli, slurm_addr_t *self,
***************
*** 1902,1907 ****
--- 2027,2072 ----
  }
  
  static int
+ _rpc_sim_job(slurm_msg_t *msg)
+ {
+ 	int        rc = SLURM_SUCCESS;
+     sim_job_msg_t *sim_job;
+     simulator_event_info_t *new;
+     static int last_job_id; /* Controlling failed submissions */
+ 
+     sim_job = (sim_job_msg_t *)msg->data;
+ 
+     info("SIM: Got info for jobid: %u with a duration of %u\n", sim_job->job_id, sim_job->duration);
+ 
+     if(sim_job->job_id != last_job_id){
+         new = (simulator_event_info_t *)calloc(1, sizeof(simulator_event_info_t));
+         if(!new){
+             info("SIM: _rpc_sim_job error in calloc\n");
+             return rc;
+         }
+ 
+         new->job_id = sim_job->job_id;
+         new->duration = sim_job->duration;
+ 
+         new->next = head_simulator_event_info;
+         head_simulator_event_info = new;
+ 
+         last_job_id = sim_job->job_id;
+     }
+     else{
+         info("SIM: Got an existent job_id. Updating duration for last job inserted\n");
+         head_simulator_event_info->duration = sim_job->duration;
+     }
+ 
+ 	if (slurm_send_rc_msg(msg, rc) < 0) {
+ 		error("Error responding to sim_job: %m");
+ 	}
+     return rc;
+ }
+ 
+ 
+ 
+  static int
  _rpc_ping(slurm_msg_t *msg)
  {
  	int        rc = SLURM_SUCCESS;
***************
*** 2162,2167 ****
--- 2327,2411 ----
  }
  
  static void
+ simulator_rpc_terminate_job(slurm_msg_t *rec_msg)
+ {
+ 
+ 	slurm_msg_t            msg;
+ 	epilog_complete_msg_t  req;
+     hostlist_t hl;
+    char *node_name;
+ 	int             rc     = SLURM_SUCCESS;
+ 	kill_job_msg_t *req_kill    = rec_msg->data;
+ 	simulator_event_t *temp, *event_sim;
+ 
+     /* First sending an OK to the controller */
+ 
+ 
+     info("simulator_rpc_terminate_job, jobid = %d", req_kill->job_id);
+ 
+     slurm_send_rc_msg(rec_msg, SLURM_SUCCESS);
+ 
+     info("simulator_rpc_terminate_job, jobid = %d ready for sending message", req_kill->job_id);
+ 
+ 	pthread_mutex_lock(&simulator_mutex);
+ 
+ 	event_sim = head_sim_completed_jobs;
+ 
+ 	if((head_sim_completed_jobs) && (head_sim_completed_jobs->job_id == req_kill->job_id)){
+ 			head_sim_completed_jobs = head_sim_completed_jobs->next;
+ 	}else{
+ 
+ 			temp = head_sim_completed_jobs;
+ 			if(!temp){
+ 					printf("SIM: Error, no event found for completed job %d\n", req_kill->job_id);
+ 					pthread_mutex_unlock(&simulator_mutex);
+ 					return;
+ 			}
+ 
+ 			while((temp->next) && (temp->next->job_id =! req_kill->job_id)){
+ 					temp = temp->next;
+ 			}
+ 
+ 			if(temp->next){
+ 					event_sim = temp->next;
+ 					temp->next = event_sim->next;
+ 			}else{
+ 					printf("SIM: Error, no event found for completed job %d\n", req_kill->job_id);
+ 					pthread_mutex_unlock(&simulator_mutex);
+ 					return;
+ 			}
+ 	}
+ 
+ 	pthread_mutex_unlock(&simulator_mutex);
+ 
+     hl = hostlist_create(event_sim->nodelist);
+ 
+ 	/* With FRONTEND just one epilog complete message is needed */
+     node_name = hostlist_shift(hl);
+     //while ((node_name = hostlist_shift(hl))) {
+ 
+       info("SIM: Sending epilog complete message for job %d node %s", req_kill->job_id, node_name);
+ 	    slurm_msg_t_init(&msg);
+ 
+ 	    req.job_id      = req_kill->job_id;
+ 	    req.return_code = rc;
+ 	    req.node_name   = node_name;
+ 	    if (switch_g_alloc_node_info(&req.switch_nodeinfo))
+ 		    error("switch_g_alloc_node_info: %m");
+ 
+ 	    msg.msg_type    = MESSAGE_EPILOG_COMPLETE;
+ 	    msg.data        = &req;
+ 
+ 		/* Let wait for an answer for simulation syncronization */
+ 		slurm_send_recv_controller_rc_msg(&msg, &rc);
+ 
+ 	    switch_g_free_node_info(&req.switch_nodeinfo);
+     //}
+    hostlist_destroy(hl);
+ 	free(event_sim);
+ }
+ 
+ static void
  _rpc_terminate_tasks(slurm_msg_t *msg)
  {
  	kill_tasks_msg_t *req = (kill_tasks_msg_t *) msg->data;
